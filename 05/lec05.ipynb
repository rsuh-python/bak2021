{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ad6d00-27c9-47d0-bbb7-c4ece9d076b5",
   "metadata": {},
   "source": [
    "### Морфопарсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d822ac-29f4-4539-8e81-935bb48bfab7",
   "metadata": {},
   "source": [
    "Установка примерно всех библиотек, которые нам понадобятся в жизни:\n",
    "\n",
    "Команды, выделенные курсивом, выполняются в консоли, остальные в интерактивной среде питона. В Colaboratory консольные команды можно запускать в обычных ячейках с восклицательным знаком:\n",
    "\n",
    "    !pip install modulename\n",
    "\n",
    "**NLTK**\n",
    "\n",
    "*pip install nltk*\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "    \n",
    "В появившемся окне загрузки выбрать all и загрузить. Может быть довольно долго! \n",
    "\n",
    "*Colab*: предустановлен, но дополнительные штуки не загружены (с их загрузкой могут быть проблемы &ndash; в колабе нет графического окна для нее). \n",
    "\n",
    "**spaCy**\n",
    "\n",
    "*pip install spacy*\n",
    "\n",
    "*python -m spacy download _modelname_*  # список возможных моделей и сама команда доступны на [официальном сайте спейси](https://spacy.io/usage/models)\n",
    "\n",
    "Нам еще понадобится [обертка udpipe для spacy](https://spacy.io/universe/project/spacy-udpipe):\n",
    "\n",
    "*pip install spacy-udpipe*\n",
    "\n",
    "*Colab*: библиотека spacy предустановлена, может понадобиться загрузить модель (т.е. использовать только вторую команду)\n",
    "\n",
    "**natasha (подмодуль razdel)**\n",
    "\n",
    "*pip install razdel* \n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**DeepPavlov (ru_sent_tokenize)**\n",
    "\n",
    "*pip install rusenttokenize*\n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**pymorphy2**\n",
    "\n",
    "*pip install pymorphy2*\n",
    "\n",
    "*pip install -U pymorphy2-dicts-ru*  # необязательно: для обновления словаря\n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**Mystem**\n",
    "\n",
    "*pip install git+https://github.com/nlpub/pymystem3 *\n",
    "\n",
    "*Colab*: гарантированно не работает. \n",
    "\n",
    "**rnnmorph**\n",
    "\n",
    "*pip install rnnmorph*\n",
    "\n",
    "Может козлить во время установки (вы сами видели...). Иногда, если плохо установился и не работает, приходится его удалять командой pip uninstall rnnmorph (обязательно в консоли от имени администратора!). Периодически, если обновляется версия tensorflow, может перестать работать &ndash; но Илья Гусев обычно вскоре обновляет и свой парсер, так что достаточно следить за новостями в [его гитхабе](https://github.com/IlyaGusev/rnnmorph) &ndash; или просто подождать светлых времен.\n",
    "\n",
    "*Colab*: обычно работает без проблем.\n",
    "\n",
    "**pyconll**\n",
    "\n",
    "*pip install pyconll*\n",
    "\n",
    "*Colab*: устанавливается без проблем. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbff4eb-c9fc-4447-a001-05370105cbcb",
   "metadata": {},
   "source": [
    "Почти все нижеописанные инструменты в питоне устроены довольно однообразно: имеется основной класс \"парсер\", экземпляр которого вам нужно создать, прежде чем что-то парсить. То есть, условно говоря, из набора машинок берете ту конкретную, которая вас повезет. Когда создан экземпляр класса, ему уже можно скармливать свои тексты. \n",
    "\n",
    "Стемминг &ndash; это уже чисто историческое, можно сказать, явление: в 1980-х, когда еще не было даже графического интерфейса у компьютеров и тем более средств автоматического морфоразбора, Мартин Портер разработал свой алгоритм стемминга: усечение окончания от псевдоосновы. Этот алгоритм так и называется \"стеммер Портера\" и доступен в версиях для нескольких европейских языков, в т.ч. для русского (Snowball &ndash; чуть более новая версия). Алгоритм с помощью правил отсекает окончания и суффиксы, основываясь на особенностях языка. Как все правиловое, работает не без ошибок. \n",
    "\n",
    "Код просто посмотреть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82ba707-6210-4f18-8267-6a1697547662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пердикк\n",
      "не\n",
      "мен\n",
      "десят\n",
      "раз\n",
      "заключа\n",
      "и\n",
      "расторга\n",
      "союз\n",
      "с\n",
      "основн\n",
      "участник\n",
      "войн\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')  # экземпляр класса \n",
    "example = ['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n",
    "for token in example:\n",
    "    print(stemmer.stem(token))  # stem() - метод класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c1d4f-fc75-4b9c-8067-79a349cae59e",
   "metadata": {},
   "source": [
    "Самые простые &ndash; правиловые морфопарсеры. Для русского языка их два: pymorphy2 и pymystem3. Pymorphy был создан Михаилом Коробовым (вот его известная [статья на хабре](https://habr.com/ru/post/176575/)) как аналог Майстем. Он работает на словаре и использует тагсет [OpenCorpora](http://opencorpora.org/)), а также статистику, предпосчитанную на этом корпусе. \n",
    "Как устроен pymorphy2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39fea396-11bb-45e4-b911-246e402c1d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn sing,gent'), normal_form='студентка', score=0.6, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 1),)),\n",
       " Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn plur,nomn'), normal_form='студентка', score=0.4, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 7),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "parse = morph.parse('студентки')\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119a386-348d-4eb1-ac2d-93cc4b5ffdfe",
   "metadata": {},
   "source": [
    "У класса MorphAnalyzer() есть метод parse, который возвращает что? Список экземпляров класса Parse. У этого класса есть свои атрибуты: word (исходная форма слова), tag (грам. инфа), normal_form (лемма), score(предпосчитанная на OpenCorpora вероятность правильности разбора) и несколько технических. \n",
    "\n",
    "Соответственно, получить информацию можно, просто обращаясь к атрибутам (не забудьте, что у нас всегда список, поэтому нужно еще и индекс разбора указывать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fce79d-dd4c-418f-a694-843aec9704d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "студентки\n",
      "NOUN,anim,femn sing,gent\n",
      "студентка\n"
     ]
    }
   ],
   "source": [
    "print(parse[0].word)\n",
    "print(parse[0].tag)\n",
    "print(parse[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cb207-a42f-493d-9a1e-8815c4c00fb0",
   "metadata": {},
   "source": [
    "Атрибут tag &ndash; это экземпляр класса OpencorporaTag, как можно догадаться. У него есть еще свои атрибуты, к которым тоже можно обращаться, чтобы получать более конкретную информацию о слове. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e809ec-18f6-4f60-a705-d8eae5daa80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: NOUN\n",
      "Одушевленность: anim\n",
      "Падеж: nomn\n",
      "Род: masc\n",
      "Наклонение: None\n",
      "Число: sing\n",
      "Лицо: None\n",
      "Время: None\n",
      "Переходность: None\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('участник')\n",
    "\n",
    "t = parse[0].tag  # я записала в переменную, просто чтобы не копировать каждый раз все целиком\n",
    "# но это то же самое, что parse[0].tag.animacy...\n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\nНаклонение: {t.mood}\\\n",
    "\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b719247-f9f6-4095-a8f4-e20c46c90cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: VERB\n",
      "Одушевленность: None\n",
      "Падеж: None\n",
      "Род: None\n",
      "Наклонение: indc\n",
      "Число: sing\n",
      "Лицо: 3per\n",
      "Время: pres\n",
      "Переходность: tran\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('говорит')\n",
    "\n",
    "t = parse[0].tag  \n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\n\\\n",
    "Наклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b22fac-181a-45ee-a0fb-505aa7e46768",
   "metadata": {},
   "source": [
    "Если вы запрашиваете категорию, которой у данного слова нет (ну нет переходности у существительного), вернется None. \n",
    "\n",
    "Также можно попросить pymorphy поставить слово в конкретную форму или вообще вернуть всю парадигму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d74d035-4a38-4f13-b1d9-87045d7d64e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='говорят', tag=OpencorporaTag('VERB,impf,tran plur,3per,pres,indc'), normal_form='говорить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'говорят', 415, 6),))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fb2fa7-f23c-4723-ab98-505dcb7fcbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='участник', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участник', 2, 0),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 1),)),\n",
       " Parse(word='участнику', tag=OpencorporaTag('NOUN,anim,masc sing,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнику', 2, 2),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 3),)),\n",
       " Parse(word='участником', tag=OpencorporaTag('NOUN,anim,masc sing,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участником', 2, 4),)),\n",
       " Parse(word='участнике', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнике', 2, 5),)),\n",
       " Parse(word='участники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участники', 2, 6),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 7),)),\n",
       " Parse(word='участникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участникам', 2, 8),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 9),)),\n",
       " Parse(word='участниками', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниками', 2, 10),)),\n",
       " Parse(word='участниках', tag=OpencorporaTag('NOUN,anim,masc plur,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниках', 2, 11),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].lexeme  \n",
    "# парадигму глагола лучше не выводить - она длинная; я перед запуском этой ячейки перезапустила разбор с существительным, поэтому не удивляйтесь. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ffa04-1667-4b11-b302-61a3dc3fd296",
   "metadata": {},
   "source": [
    "Наконец, можно попросить pymorphy выводить грам. информацию по-русски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5724ad-4d21-429b-85ef-352fa7afa017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,од,мр ед,им'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1834d273-fc7b-4ad2-8a96-bbbca5d171d8",
   "metadata": {},
   "source": [
    "Pymorphy очень быстро работает и имеет много возможностей, но совершенно не умеет разрешать омонимию и никак не учитывает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae719b-4ad8-4354-8d13-68e7fe12aeb5",
   "metadata": {},
   "source": [
    "Алгоритм, легший в основу Mystem, разрабатывался в ИППИ и был первым вообще для русского языка; его в свое время купил у них Илья Сегалович, доработал, опубликовал собственную статью. Поисковик Яндекса когда-то работал на майстеме. Сам парсер написан в С (для скорости: бинарный поиск в питоне реализовать можно только с внешними библиотеками на С, а у майстема 2 словаря, по которым нужно искать). Для питона под него сделана оболочка (pymystem3). Майстем капризный, тяжело запускается, имеет не так много функций, но работает тоже довольно быстро и умеет доносить на бастардов: сообщать, что слово не найдено в его словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a60ebb42-0341-47c0-8068-673487112e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "\n",
    "m = pymystem3.Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea18ee3-5f9f-4edf-86e1-a9c269da1958",
   "metadata": {},
   "source": [
    "Майстем принимает только сырой текст в виде одной строки: у него встроенный токенизатор, потому что он пытается учитывать контекст. Поэтому стоит указывать entire_input=False при создании экземпляра класса, чтобы он не выводил вообще все, включая пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e0713e6-5460-4f00-bff6-c134accf125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = '''Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э. После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка. На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере. На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах. С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом. Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d88047af-91ee-4fec-ac70-a05c3260fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = m.lemmatize(raw)\n",
    "analysis = m.analyze(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a2bdab-45b0-409d-b24c-465bdcba7eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пердикк',\n",
       " 'II',\n",
       " 'др',\n",
       " 'греча',\n",
       " 'Περδίκκας',\n",
       " 'Β',\n",
       " 'της',\n",
       " 'Μακεδονίας',\n",
       " 'македонский',\n",
       " 'царь']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:10]  # надеюсь, греча вас тоже порадовала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e4355f9-cd44-4c35-9f37-03170d25a203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'пердикк',\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Пердикка'},\n",
       " {'analysis': [], 'text': 'II'},\n",
       " {'analysis': [{'lex': 'др', 'gr': 'S,сокр,мн,неод=(пр|вин|дат|род|твор|им)'}],\n",
       "  'text': 'др'},\n",
       " {'analysis': [{'lex': 'греча', 'gr': 'S,жен,неод=род,мн'}], 'text': 'греч'},\n",
       " {'analysis': [], 'text': 'Περδίκκας'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a6e5a-7ac3-4340-8a75-cc860e7c88b0",
   "metadata": {},
   "source": [
    "С леммами вроде все должно быть понятно, а что зашито в анализе?\n",
    "\n",
    "Майстем возвращает список. Каждый токен в этом списке - это словарь с ключами analysis & text. Первого ключа может не быть: если у нас знак пунктуации. Если же он есть, то в нем содержится список (обычно состоящий из одного элемента - если не указать при создании экземпляра класса Mystem glue_grammar_info=False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb04f65c-b109-493d-ba4f-9b8a20073244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово: {'analysis': [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}], 'text': 'Пердикка'}\n",
      "Его грам. инфа: [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}]\n",
      "Его оригинальная форма: Пердикка\n",
      "Какие есть ключи в словаре с разбором: dict_keys(['lex', 'qual', 'gr'])\n",
      "Лемма: пердикк\n",
      "Этот ключ бывает только тогда, когда слова нет в словаре: bastard\n",
      "А это грам. информация: S,имя,муж,од=(вин,ед|род,ед)\n"
     ]
    }
   ],
   "source": [
    "print(f'Первое слово: {analysis[0]}')\n",
    "print(f\"Его грам. инфа: {analysis[0]['analysis']}\\nЕго оригинальная форма: {analysis[0]['text']}\")\n",
    "print(f\"Какие есть ключи в словаре с разбором: {analysis[0]['analysis'][0].keys()}\")\n",
    "print(f\"Лемма: {analysis[0]['analysis'][0]['lex']}\\n\\\n",
    "Этот ключ бывает только тогда, когда слова нет в словаре: {analysis[0]['analysis'][0]['qual']}\\n\\\n",
    "А это грам. информация: {analysis[0]['analysis'][0]['gr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0eec8-7d18-4616-bd23-9757e95feb4d",
   "metadata": {},
   "source": [
    "Непросто, да. Еще сложнее устроен ключ 'gr', который содержит грамматическую информацию о слове: обычно майстем склеивает варианты разбора, то есть, выше запись следует читать как \"существительное, имя собственное, мужского рода, одушевленное; возможно, Acc Sg, а возможно, Gen Sg. \n",
    "\n",
    "Вот как раз если указать, чтобы грам. информация не склеивалась, майстем будет возвращать несколько словарей с вариантами по отдельности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04664eb9-2226-40dc-92ce-edbbf52e9089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': [{'lex': 'пердикк',\n",
       "   'qual': 'bastard',\n",
       "   'gr': 'S,имя,муж,од=вин,ед'},\n",
       "  {'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=род,ед'}],\n",
       " 'text': 'Пердикка'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_noglue = pymystem3.Mystem(entire_input=False, glue_grammar_info=False)\n",
    "\n",
    "noglueanalysis = m_noglue.analyze(raw)\n",
    "noglueanalysis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b14931-cdc6-41a0-89d0-aca88d0df434",
   "metadata": {},
   "source": [
    "Теперь о вещах, которых нет в стабильной версии Mystem, а есть только в той, которая устанавливается через git:\n",
    "\n",
    "1. Майстем очень плохо умеет обрабатывать \\n. Когда он получает строку, в которой много \\n (а это неизбежно, ведь мы чаще хотим обрабатывать длиннющие тексты), на каждом \\n он перезапускает свой бинарник (написанный в С). Поэтому на длинных текстах работать будет ОЧЕНЬ медленно (впрочем, все равно быстрее нейронок...). Чтобы решить эту проблему - ведь замена \\n на пробелы, например, искажает контекст - сделали возможность особым образом обрабатывать \\n, когда загружаем текст из файла. \n",
    "2. Есть функция, которая позволяет получить часть речи для конкретного токена. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b8b59-a5ec-4fd3-b8ff-5dd954e3cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = m.analyze(file_path=path) # можно напрямую передавать в майстем путь к файлу с текстом - он сам откроет и обработает как ему надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73a22b49-6a77-4edf-b268-1358fe0c6adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_pos(analysis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b91740-5b79-4365-87af-2bdc94d69841",
   "metadata": {},
   "source": [
    "В 2017 году на соревновании конференции \"Диалог\" победила команда Гусев-Анастасьев: ребята создали морфопарсер для русского языка на нейронной сети. Он называется rnnmorph (RNN - это название модели нейронной сети, на которой он работает). Гусев при создании явно вдохновлялся pymorphy2 (кстати, Коробов для этого же соревнования сделал библиотеку для приведения разных тагсетов к одному): синтаксис rnnmorph очень похож на pymorphy2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e210b2a-10ca-41ab-b1a8-41e299433b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 12:00:12.010260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:12.010289: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-06 12:00:13.910063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:13.910109: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-06 12:00:13.910134: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aslin): /proc/driver/nvidia/version does not exist\n",
      "2022-04-06 12:00:13.910346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<normal_form=пердикк; word=Пердикка; pos=NOUN; tag=Case=Acc|Gender=Masc|Number=Sing; score=0.7922>,\n",
       " <normal_form=не; word=не; pos=PART; tag=_; score=1.0000>,\n",
       " <normal_form=менее; word=менее; pos=ADV; tag=Degree=Cmp; score=1.0000>,\n",
       " <normal_form=десять; word=десяти; pos=NUM; tag=Case=Gen; score=1.0000>,\n",
       " <normal_form=раз; word=раз; pos=NOUN; tag=Case=Gen|Gender=Masc|Number=Plur; score=0.9998>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "\n",
    "predictor = RNNMorphPredictor(language='ru')\n",
    "parse = predictor.predict(['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.'])\n",
    "parse[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c184fce-6952-4b09-afa9-51dccf349211",
   "metadata": {},
   "source": [
    "rnnmorph принимает список строк (токенов) и возвращает список объектов специального класса, у которого есть атрибуты normal_form (лемма), word (исходная форма), pos (часть речи), tag (грам. информация) и score (уверенность нейронной модели в правильности своего ответа). Он умеет снимать омонимию (лучше, чем майстем, но хуже, чем интегральный морфопарсер). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2087b660-f45e-4465-96d4-e3556345b945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[-2].pos  # например, можно узнать часть речи для предпоследнего слова в списке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31409e66-d70f-4e72-97ef-ca31974f8d62",
   "metadata": {},
   "source": [
    "*Примечание*\n",
    "\n",
    "RNNMorph, как и любая нейронная модель, умеет работать на GPU (видеокарте). \n",
    "\n",
    "Для любопытных: нейронные сети &ndash; это, по сути, бесконечное умножение гигантских матриц друг на друга. Нейронная сеть состоит из кучи нейронов-функций, и когда она должна выдать ответ, она получает входные данные в виде таких же матриц на первый слой, который состоит из миллиона нейронов, этот миллион нейрончиков кидается высчитывать результат зашитой внутри них функции (что-то похожее на ax + b), а их ответы получает второй слой таких же нейрончиков, и так пока не получится финальный ответ. То есть, мы производим миллиарды однотипных вычислений. Процессор видеокарты устроен как раз таким образом, что он супер-быстро умеет считать однотипные вещи (он считает их батчами &ndash; сразу пачками), то есть, он работает гораздо быстрее, чем CPU, но обучен именно однотипно считать. Поэтому нейронки обычно и работают на GPU, они просто созданы друг для друга!\n",
    "\n",
    "Когда нейронка (и RNNMorph тоже) работает на видеокарте, она делает это обычно быстрее, чем на CPU. Но чтобы заставить RNNMorph перейти на видеокарту, нужно сложно настраивать ее (и иметь совместимую видеокарту до кучи); поэтому RNNMorph умеет, конечно, работать и на обычном процессоре, но при этом вываливает предупреждения про то, что настройки видеокарты он не нашел (они выше выделены красным). Их можно смело игнорировать! Это предупреждения, а не ошибки. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d6a16-d1ec-4e7f-8207-9161da685ecc",
   "metadata": {},
   "source": [
    "Последний парсер, который мы посмотрели &ndash; это интегральный морфопарсер Анастасьева, победивший в соревнованиях в 2020 году. Его особенность в том, что он работает в формате Universal Dependencies и делает одновременно морфо- и синтаксический разбор. Поэтому, прежде чем перейти к нему, посмотрим библиотеку для просмотра файлов формата .conllu &ndash; в нем записывается UD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a4c45-396d-44b2-b483-d58add6d148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "text = pyconll.load_from_file('myfile.conllu')\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        print(token.id, token.form, token.lemma, token.upos, token.feats, token.head, token.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf157d-8a00-49d1-ab45-6939ea67eff0",
   "metadata": {},
   "source": [
    "Естественно, можно не только печатать информацию, но и добавлять в список и вообще делать все, что угодно. Что это за атрибуты у токенов?\n",
    "\n",
    "- id - порядковый номер токена в предложении\n",
    "- form - исходная форма\n",
    "- lemma - лемма\n",
    "- upos - часть речи \n",
    "- feats - грам. характеристики\n",
    "- head - расстояние от синтаксической вершины\n",
    "- deprel - тип синтаксической связи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad0cbf-867f-4c34-bed7-0927e8234806",
   "metadata": {},
   "source": [
    "Где можно красивенько отрисовать .conllu файлы в виде деревьев зависимости:\n",
    "\n",
    "[Арборатор](https://arborator.ilpga.fr/q.cgi): достаточно вставить текст в формате .conllu\n",
    "\n",
    "[Conllu-Viewer на сайте UD](https://universaldependencies.org/conllu_viewer.html): умеет читать файлы и рисовать последовательно все предложения\n",
    "\n",
    "Для затравки вот картинка с арборатора:\n",
    "\n",
    "<img src='arbo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690140d5-1588-4a11-9a1c-e3be3e588904",
   "metadata": {},
   "source": [
    "Сам парсер Анастасьев, к сожалению, так и не собрал в устанавливаемый модуль, но простые лингвисты могут воспользоваться его тетрадкой (я ее дорабатывала немного, потому что совместимости со временем страдают): [Колаб с интегральным морфопарсером](https://colab.research.google.com/drive/1nuIqJ-YUuDihSzi37A-YLy2uW7Ll-2Rr?usp=sharing)\n",
    "\n",
    "(Скопируйте эту тетрадку себе). \n",
    "\n",
    "Как ей пользоваться?\n",
    "\n",
    "1. Запустить ячейку Dependencies. \n",
    "2. Пока она выполняется, можно успеть подготовить свой пустой .conllu для заполнения... Бдите, чтобы не было слишком длинных предложений!\n",
    "3. Загрузить свой файл в любую папку в колаб. \n",
    "4. Указать путь к этой папке в ячейке Apply: \n",
    "\n",
    "        !cd GramEval2020 \\\n",
    "            && ./download_model.sh ru_bert_final_model \\\n",
    "            && cd solution \\\n",
    "            && python -m train.applier --data-dir /content/test --model-name ru_bert_final_model --batch-size 8 --pretrained-models-dir ../pretrained_models\n",
    "            \n",
    "       Здесь вместо /content/test должно быть имя вашей папки (/content - это сама директория, поэтому если заводите новую папку, то будет /content/ваша папка)\n",
    "       \n",
    "5. Можно дополнительно выбрать другую модель из списка в [гитхабе Анастасьева](https://github.com/DanAnastasyev/GramEval2020), но они не все рабочие. \n",
    "6. Запустить Apply! \n",
    "\n",
    "Если начнут вываливаться ошибки, гуглите: могли полететь совместимости. Если выдает ошибку про \"мало памяти, дай ищо\", проверьте свой файл на длинные предложения!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
